{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-12 03:17:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt.5'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 4.78M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 10.3M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 6.35M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 12.4M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 9.68M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 29.4M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 61.1M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36% 12.8M 0s\n",
      "   400K .......... .......... .......... .......... .......... 41%  139M 0s\n",
      "   450K .......... .......... .......... .......... .......... 45%  111M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50% 11.6M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55% 3.89M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59%  104M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64%  174M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68% 12.5M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73%  129M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78%  127M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82%  169M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87%  204M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91%  189M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96% 3.92M 0s\n",
      "  1050K .......... .......... .......... .........            100%  162M=0.07s\n",
      "\n",
      "2024-04-12 03:17:10 (14.3 MB/s) - 'input.txt.5' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))\n",
    "# inspect the first 100 characters to sanity check\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# get the alphabet\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab:', ''.join(chars))\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: First Citi -> [18, 47, 56, 57, 58, 1, 15, 47, 58, 47]\n",
      "decoded: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47] -> First Citi\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)} # map from character to integer\n",
    "itos = {i:ch for i,ch in enumerate(chars)} # map from integer to character\n",
    "encode = lambda s: [stoi[c] for c in s] # encode a string into a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # other way around\n",
    "\n",
    "# sanity check\n",
    "print('encoded:', text[:10], '->', encode(text[:10]))\n",
    "print('decoded:', encode(text[:10]), '->', decode(encode(text[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1115394]) dtype: torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"shape:\", data.shape, \"dtype:\", data.dtype)\n",
    "print(data[:10])\n",
    "# observe how it's the same as encode(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validaton sets\n",
    "n = int(len(data)*0.9) # first 90% is training data, remaining is validation data\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "# the first 9 characters of the training set\n",
    "# 9 because there are \"8\" pairs of examples\n",
    "# this snippet of code illustrates what the 8 examples are\n",
    "\n",
    "# we train all 8 examples not just because it's computationally convenient and efficient\n",
    "# this also trains the model on different lengths of text\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8]) tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: torch.Size([4, 8]) tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24], target is 43\n",
      "when input is [24, 43], target is 58\n",
      "when input is [24, 43, 58], target is 5\n",
      "when input is [24, 43, 58, 5], target is 57\n",
      "when input is [24, 43, 58, 5, 57], target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1], target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], target is 39\n",
      "when input is [44], target is 53\n",
      "when input is [44, 53], target is 56\n",
      "when input is [44, 53, 56], target is 1\n",
      "when input is [44, 53, 56, 1], target is 58\n",
      "when input is [44, 53, 56, 1, 58], target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46], target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], target is 1\n",
      "when input is [52], target is 58\n",
      "when input is [52, 58], target is 1\n",
      "when input is [52, 58, 1], target is 58\n",
      "when input is [52, 58, 1, 58], target is 46\n",
      "when input is [52, 58, 1, 58, 46], target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39], target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], target is 46\n",
      "when input is [25], target is 17\n",
      "when input is [25, 17], target is 27\n",
      "when input is [25, 17, 27], target is 10\n",
      "when input is [25, 17, 27, 10], target is 0\n",
      "when input is [25, 17, 27, 10, 0], target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21], target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate 4 random starting indices\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # construct the inputs\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # construct the targets\n",
    "    # torch.stack stacks the tensors as rows in a new 4 by 8 tensor\n",
    "    # 4 rows for 4 examples, 8 columns for 8 characters in each example\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:', xb.shape, xb)\n",
    "print('targets:', yb.shape, yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()}, target is {target}\")\n",
    "    \n",
    "# that's 4*8 = 32 training examples in a single batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # example input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# start basic with a bigram model\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # nn.Embedding is a thin wrapper around a (vocab_size, vocab_size) tensor\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integer\n",
    "        # (batch_size, block_size)\n",
    "        # every integer in `idx` going to \"pluck\" out a row from the embedding table\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) where C is channel = vocab_size\n",
    "        \n",
    "        # we'll treat logits as the prediction of the next token, where logits[b,t,c] is the guess? probability \n",
    "        # that letter decode(c) is the next letter, after seeing letter decode(idx[b,t])\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # cross entropy loss, the standard loss function for classification\n",
    "            # if we have multidimensional input, pytorch wants the channels to be the second dimension\n",
    "            # we don't want to deal with that, so we flatten the batch and time dimensions together\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices is the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            #self(idx) calls the forward method\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            # -1 means the last element in the time dimension, we pluck it out\n",
    "            last_logits = logits[:, -1, :] # (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(last_logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            # each batch dimension will have a single prediction\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append the sampled token to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb) # (B, T, C), prediction for each character in xb\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "# we expect -ln(1/65) loss \n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long) # (1,1) tensor with a single 0, 0 is a newline character\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optmizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# AdamW is an advanced and popular optimizer.\n",
    "# SGD (stochastic gradient descent) is a simpler optimizer that works well too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 3.1474289894104004\n",
      "step 1000, loss 2.731760263442993\n",
      "step 2000, loss 2.6109120845794678\n",
      "step 3000, loss 2.587460994720459\n",
      "step 4000, loss 2.5016682147979736\n",
      "step 5000, loss 2.455209493637085\n",
      "step 6000, loss 2.3881759643554688\n",
      "step 7000, loss 2.4599270820617676\n",
      "step 8000, loss 2.4092462062835693\n",
      "step 9000, loss 2.3968563079833984\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # get a batch\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if steps % 1000 == 0:\n",
    "        print(f'step {steps}, loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An, yhe' m ane! :\n",
      "Hobsad, s IAle h mexK:\n",
      "We, makisoung, hall-hithin p wate st--\n",
      "TE:\n",
      "Ifur met th hire onsteiahe cour, RDilothay Mube t VOLUKERUS:\n",
      "\n",
      "ABEXExe s s hpr ug y'd it trr,\n",
      "I lay:\n",
      "EOMy athaveanghur amex.\n",
      "Whes:\n",
      "F: I\n",
      "\n",
      "Buru.\n",
      "Serth'Whth:\n",
      "AUESI isin. thed thmyo as ELI g.\n",
      "CHUSisthethellend:\n",
      "GAnd fo,\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # (1,1) tensor with a single 0, 0 is a newline character\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
