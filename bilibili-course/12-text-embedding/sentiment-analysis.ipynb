{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "This notebook does sentiment analysis on a dataset of chinese ecommerce reviews.\n",
    "\n",
    "We use a special chinese tokenizer `jieba` to tokenize the reviews, embed the tokens using an `Embedding` layer, then use a simple fully connected neural network to predict the sentiment of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 03:20:23.374414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 03:20:23.989176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import data_loader\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size:16595\n",
      "Train set size:13276\n",
      "Test set size:3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.541 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca: 9512\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data_loader.load_data()\n",
    "vocalen, word_index = data_loader.createWordIndex(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ix = data_loader.word2Index(x_train, word_index)\n",
    "x_test_ix = data_loader.word2Index(x_test, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TensorDataset(torch.from_numpy(x_train_ix), torch.from_numpy(y_train[:, np.newaxis]))\n",
    "test = data.TensorDataset(torch.from_numpy(x_test_ix), torch.from_numpy(y_test[:, np.newaxis]))\n",
    "train_loader = data.DataLoader(train, batch_size=512, shuffle=True)\n",
    "test_loader = data.DataLoader(test, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=256, num_layers=3):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.sequential.add_module(f\"linear{_}\", nn.LazyLinear(hidden_dim))\n",
    "            self.sequential.add_module(f\"relu{_}\", nn.ReLU())\n",
    "        \n",
    "        self.sequential.add_module(\"linear_final\", nn.LazyLinear(1))\n",
    "        self.sequential.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(L.LightningModule):\n",
    "    def __init__(self, vocalen, embed_dim=256, hidden_dim=256, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.model = Model(vocalen, embed_dim, hidden_dim, num_layers)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        x, y = batch\n",
    "        preds = self.model(x)\n",
    "        loss = F.binary_cross_entropy(preds, y)\n",
    "        acc = (preds.round() == y).float().mean()\n",
    "        \n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wenqi/.local/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:461: The total number of parameters detected may be inaccurate because the model contains an instance of `UninitializedParameter`. To get an accurate number, set `self.example_input_array` in your LightningModule.\n",
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 2.4 M  | train\n",
      "----------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.740     Total estimated model params size (MB)\n",
      "/home/wenqi/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/wenqi/.local/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c0105a417c4207b57b8e633b318390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    }
   ],
   "source": [
    "model = Module(vocalen)\n",
    "trainer = L.Trainer(max_epochs=200)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
